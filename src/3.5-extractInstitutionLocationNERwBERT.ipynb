{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecea809d-30e1-4936-bc60-b3235caf7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Set max column width\n",
    "pd.set_option(\"max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef27f5de-7cc7-415c-940f-064212251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\temp_isna_notna_base\\\n"
     ]
    }
   ],
   "source": [
    "# Input directory (dir. with csvs containing parsed articles)\n",
    "DF_input = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495a21dc-a4e3-4392-a5ca-be5389251fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " D:\\Data_40_years_cancer_studies\\BERT_NER_parsedXMLs_temp_isna_notna_base\\\n"
     ]
    }
   ],
   "source": [
    "DF_output = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908a5980-5b07-4c36-b701-b538ec78ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_csvs = []\n",
    "\n",
    "for file in os.listdir(DF_input):\n",
    "    if file[-4:] == \".csv\":\n",
    "        list_csvs.append(file)\n",
    "\n",
    "n_csvs = len(list_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2133361-3be4-466e-83c9-2dc41148c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\bert-fineutned-ner-4511-optuna-optimized\\\n"
     ]
    }
   ],
   "source": [
    "# Input directory for the BERT model fine tuned for affiliation NER\n",
    "path_BERT = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5bd722-ed4c-4c68-8fd8-78c17684a895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model\n",
    "model = AutoModelForTokenClassification.from_pretrained(path_BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "734d29fb-b83d-49db-a4a5-0e46bccb4df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting affiliation of articles in csv: temp_isna_notna.csv (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 141701/141701 [2:03:21<00:00, 19.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract the affiliation of the last author (or, if that is not available, of any author) of each article\n",
    "# Then, do NER on the affiliation, add result to dataframe and save it\n",
    "no_affiliation = []\n",
    "parsed_csvs = []\n",
    "\n",
    "for csv in list_csvs:\n",
    "    if csv not in parsed_csvs: \n",
    "        start = time.time()\n",
    "        NER_input = {} # PMIDs used as keys, affiliation for this PMID as values\n",
    "        NER_output = {} # PMIDs used as keys, NER from BERT as values\n",
    "\n",
    "        # Part 1: Extract affiliation of last (or, if not available, any other) author of each article\n",
    "        print(f\"Extracting affiliation of articles in csv: {csv} ({str(list_csvs.index(csv)+1)}/{str(n_csvs)})\")\n",
    "        df= pd.read_csv(DF_input + csv)\n",
    "        for i in range(df.index[-1] + 1): # Iterate through all rows, including the last one\n",
    "            NER_input[df.at[i, \"PMID\"]] = []\n",
    "            try:\n",
    "                authors_data = ast.literal_eval(df.at[i, \"Authors\"])\n",
    "                for author in authors_data:\n",
    "                    if \"Affiliation\" in author and author[\"Affiliation\"]:\n",
    "                        for affiliation in author[\"Affiliation\"]:\n",
    "                            NER_input[df.at[i, \"PMID\"]].append(affiliation)\n",
    "\n",
    "                if len(NER_input[df.at[i, \"PMID\"]]) == 0:\n",
    "                    no_affiliation.append(df.at[i, \"PMID\"])\n",
    "\n",
    "            except Error:\n",
    "                NER_input[df.at[i, \"PMID\"]] = None\n",
    "                no_affiliation.append(df.at[i, \"PMID\"])\n",
    "\n",
    "        for key in NER_input.keys():\n",
    "            NER_input[key] = list(set(NER_input[key]))\n",
    "    \n",
    "            if NER_input[key] == []:\n",
    "                NER_input[key] == None\n",
    "    \n",
    "            else:\n",
    "                combined_input = \"\"\n",
    "                for affiliation in NER_input[key]:\n",
    "                    combined_input += \". \" + affiliation\n",
    "        \n",
    "                NER_input[key] = combined_input[2:]\n",
    "\n",
    "        # Part 2: Do NER on the affiliation to extract structured info\n",
    "        for key in tqdm(NER_input.keys()):\n",
    "            if len(NER_input[key]) > 0:\n",
    "                text = NER_input[key]\n",
    "                entities = nlp(text)\n",
    "                NER_output[key] = entities\n",
    "\n",
    "            else:\n",
    "                NER_output[key] = []\n",
    "\n",
    "    df_NER_BERT = pd.DataFrame(pd.Series(NER_output, name=\"values\"))\n",
    "    df_NER_BERT[\"PMID_NER\"] = df_NER_BERT.index\n",
    "    df_NER_BERT = df_NER_BERT.rename(columns={\"values\": \"NER_BERT\"}).reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "    df_save = pd.merge(df, df_NER_BERT, left_on= \"PMID\", right_on=\"PMID_NER\", how=\"left\")\n",
    "    df_save = df_save.drop(columns=[\"PMID_NER\"])\n",
    "\n",
    "    df_save.to_csv(DF_output + \"BERT_NER_\" + csv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d54e8-7a13-4725-b6b9-c1b0aa48370a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
