{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50906a7-9ad3-4ca3-a339-5376174f2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Libraries for language detection. Avoid warnings\n",
    "from langdetect import detect, DetectorFactory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='langdetect')\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# Ensure consistent language detection results across runs\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90a28d0-5b71-4e39-b033-de9759e5f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models\n",
    "\n",
    "nlp_models = {}\n",
    "nlp_models['en'] = spacy.load(\"en_core_web_lg\")\n",
    "nlp_models['es'] = spacy.load(\"es_core_news_lg\")\n",
    "nlp_models['fr'] = spacy.load(\"fr_core_news_lg\")\n",
    "nlp_models['de'] = spacy.load(\"de_core_news_lg\")\n",
    "nlp_models['it'] = spacy.load(\"it_core_news_lg\")\n",
    "nlp_models['ja'] = spacy.load(\"fr_core_news_lg\")\n",
    "nlp_models['pl'] = spacy.load(\"pl_core_news_lg\")\n",
    "nlp_models['pt'] = spacy.load(\"pt_core_news_lg\")\n",
    "nlp_models['ru'] = spacy.load(\"ru_core_news_lg\")\n",
    "nlp_models['sv'] = spacy.load(\"sv_core_news_lg\")\n",
    "nlp_models['undetermined'] = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f0cca7-8ad5-4ec8-bcdf-b8f8ff8f480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\parsedXMLs_update_2025-11\\\n"
     ]
    }
   ],
   "source": [
    "# Input directory (dir. with csvs containing parsed articles)\n",
    "DF_input = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe57bf8a-af59-48ce-8c9f-a404e5fc7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_csvs = []\n",
    "\n",
    "for file in os.listdir(DF_input):\n",
    "    if file[-4:] == \".csv\":\n",
    "        list_csvs.append(file)\n",
    "\n",
    "n_csvs = len(list_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba36d537-2f52-41e7-8672-9a291227df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting affiliation of articles in csv: parsedXMLs_update_2025_66400.csv (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_18732\\3705389644.py:17: DtypeWarning: Columns (2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing NER of articles in csv: parsedXMLs_update_2025_66400.csv (1/1)\n",
      "--Processing time: 632.99 s\n"
     ]
    }
   ],
   "source": [
    "# Extract the affiliation of the last author (or, if that is not available, of any author) of each article\n",
    "# Then, do NER on the affiliation, add result to dataframe and save it\n",
    "no_affiliation = []\n",
    "parsed_csvs = []\n",
    "if os.path.exists(DF_input+\"csv files with NER.txt\"):\n",
    "    with open(DF_input+\"csv files with NER.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parsed_csvs.append(line[:-1])\n",
    "    \n",
    "for csv in list_csvs:\n",
    "    if csv not in parsed_csvs: \n",
    "        start = time.time()\n",
    "        NER_input = {} # PMIDs used as keys, affiliation for this PMID as values\n",
    "        NER_lastAuthor = {} # PMIDs used as keys, NER of affiliation as values\n",
    "        # Part 1: Extract affiliation of last (or, if not available, any other) author of each article\n",
    "        print(f\"Extracting affiliation of articles in csv: {csv} ({str(list_csvs.index(csv)+1)}/{str(n_csvs)})\")\n",
    "        df= pd.read_csv(DF_input + csv)\n",
    "        for i in range(df.index[-1] + 1): # Iterate through all rows, including the last one\n",
    "            try:\n",
    "                authors_data = ast.literal_eval(df.at[i, \"Authors\"])\n",
    "                last_author = authors_data[-1]\n",
    "        \n",
    "                affiliation_found = False\n",
    "                if \"Affiliation\" in last_author and last_author[\"Affiliation\"]:\n",
    "                    # If the last author has an \"Affiliation\" key and it's not empty,\n",
    "                    # take the first one. You can modify this to take any specific one\n",
    "                    # or iterate if you have a preference.\n",
    "                    NER_input[df.at[i,\"PMID\"]] = last_author[\"Affiliation\"][0]\n",
    "                    affiliation_found = True\n",
    "                else:\n",
    "                    # If the last author doesn't have an \"Affiliation\" key or it's empty,\n",
    "                    # try to find any affiliation within their entry.\n",
    "                    for el in authors_data:\n",
    "                        if el[\"Affiliation\"] is not None and len(el[\"Affiliation\"]) != 0:\n",
    "                            NER_input[df.at[i,\"PMID\"]] = el[\"Affiliation\"][0]\n",
    "                            affiliation_found = True\n",
    "                            break # Stop after finding the affiliation for any author                    \n",
    "        \n",
    "                if not affiliation_found:\n",
    "                    NER_input[df.at[i,\"PMID\"]] = None\n",
    "                    no_affiliation.append(df.iat[i, 1])\n",
    "        \n",
    "            except (SyntaxError, ValueError, IndexError, KeyError):\n",
    "                # Handle cases where ast.literal_eval fails, or index is out of bounds,\n",
    "                # or 'Affiliation' key is not found in the expected structure.\n",
    "                NER_input[df.at[i,\"PMID\"]] = None\n",
    "                no_affiliation.append(df.iat[i, 1])\n",
    "    \n",
    "        # Part 2: Do NER on the affiliation to extract structured info\n",
    "        print(f\"Doing NER of articles in csv: {csv} ({str(list_csvs.index(csv)+1)}/{str(n_csvs)})\")\n",
    "        for article in list(NER_input.keys()):\n",
    "            # Default language in affiliation text\n",
    "            detected_lang = \"undetermined\"\n",
    "    \n",
    "            # Attempt to detect affiliation text language\n",
    "            try:\n",
    "                detected_lang = detect(NER_input[article])\n",
    "            except Exception as e:\n",
    "                detected_lang = \"undetermined\"\n",
    "    \n",
    "            # Use the model for the detected language (or a multilingual model if language not detected or no model available for the detected language)\n",
    "            # Do NER with this model on the affiliation\n",
    "            try:\n",
    "                # Load model\n",
    "                nlp = nlp_models.get(detected_lang)\n",
    "\n",
    "                if nlp == None:\n",
    "                    nlp = nlp_models.get(\"undetermined\")\n",
    "    \n",
    "                # Load affiliation text, do NER\n",
    "                doc = nlp(NER_input[article])\n",
    "                sentence_entities = []\n",
    "    \n",
    "                # Extract entities of interest\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ in [\"ORG\", \"LOC\", \"GPE\"]:\n",
    "                        sentence_entities.append({\"text\": ent.text, \"label\": ent.label_})\n",
    "    \n",
    "                # Save the entities associated to the article DOI\n",
    "                NER_lastAuthor[article] = ({\"entities\": sentence_entities})\n",
    "    \n",
    "            # If NER not possible, store an empty dict\n",
    "            except ValueError:\n",
    "                NER_lastAuthor[article] = {}\n",
    "                \n",
    "        # Create df from dictionary\n",
    "        df_NER_lastAuthor = pd.DataFrame.from_dict(NER_lastAuthor, orient=\"index\")\n",
    "        df_NER_lastAuthor = df_NER_lastAuthor.rename(columns={\"entities\": \"NER_lastAuthor\"})\n",
    "        df_NER_lastAuthor[\"PMID_NER\"] = df_NER_lastAuthor.index\n",
    "    \n",
    "        # Merge with original df, save\n",
    "        df_save = pd.merge(df, df_NER_lastAuthor, left_on= \"PMID\", right_on=\"PMID_NER\", how=\"left\")\n",
    "        df_save = df_save.drop(columns=[\"PMID_NER\"])\n",
    "        df_save.to_csv(DF_input+csv, index=False)\n",
    "        with open(DF_input+\"csv files with NER.txt\", \"a\") as f:\n",
    "            f.write(csv+\"\\n\")\n",
    "        parsed_csvs.append(csv)\n",
    "        del df, df_save, df_NER_lastAuthor\n",
    "        gc.collect()\n",
    "        print(f\"--Processing time: {str(round(time.time() - start, 2))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19415f-4a3d-4996-b805-37fbf4f82f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
