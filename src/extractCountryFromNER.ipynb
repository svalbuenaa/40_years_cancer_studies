{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef54bae-c419-4666-b029-246b42b8b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import country_converter as coco\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import googlemaps\n",
    "from urllib.parse import quote_plus\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "# Manage API Keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key_GOOGLEMAPS = os.getenv(\"API_KEY_GoogleMaps_new\")\n",
    "user_agent_NOMINATIM = os.getenv(\"USER_AGENT_NOMINATIM\")\n",
    "\n",
    "import logging\n",
    "coco_logger = logging.getLogger('country_converter.country_converter')\n",
    "coco_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619c3c9d-1578-4823-84a5-a09824e8f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "      C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\parsedXMLs\\\n"
     ]
    }
   ],
   "source": [
    "DF_input = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bafe16a-ebab-4883-b3ee-1ae6423779d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "    C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\resources\\allCountries_clean.csv\n"
     ]
    }
   ],
   "source": [
    "entities_database_input = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458742f3-0efa-4b0a-bb61-de1d55664d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "    C:\\Users\\svalb\\OneDrive\\Escritorio\\Data_40_years_cancer_studies\\resources\\ScimagoIR 2025 - Overall Rank.csv\n"
     ]
    }
   ],
   "source": [
    "institutions_database_input = input().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e15d09-8e59-4641-94bc-7e24de50f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import databases\n",
    "entities_database = pd.read_csv(entities_database_input)\n",
    "institutions_database = pd.read_csv(institutions_database_input, sep=\";\")\n",
    "\n",
    "# Remove \" *\" from institutions when present\n",
    "institutions_database[\"Institution\"] = institutions_database[\"Institution\"].apply(lambda x: x[:-2] if x[-2:] == \" *\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e502e711-27ec-40c4-9386-41acdd2a9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of csvs to parse\n",
    "list_csvs = []\n",
    "\n",
    "for file in os.listdir(DF_input):\n",
    "    if file[-4:] == \".csv\":\n",
    "        list_csvs.append(file)\n",
    "\n",
    "n_csvs = len(list_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd3c98f-f85a-426c-88e6-e6f38940b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for US states. \n",
    "# Many article's affiliation have the state name/ code and not \"United States\"\n",
    "us_states = [\n",
    "    \"Alabama\", \"AL\", \"Al\",\n",
    "    \"Alaska\", \"AK\", \"Ak\",\n",
    "    \"Arizona\", \"AZ\", \"Az\",\n",
    "    \"Arkansas\", \"AR\", \"Ar\",\n",
    "    \"California\", \"CA\", \"Ca\",\n",
    "    \"Colorado\", \"CO\", \"Co\",\n",
    "    \"Connecticut\", \"CT\", \"Ct\",\n",
    "    \"Delaware\", \"DE\", \"De\",\n",
    "    \"Florida\", \"FL\", \"Fl\",\n",
    "    \"Georgia\", \"GA\", \"Ga\",\n",
    "    \"Hawaii\", \"HI\", \"Hi\",\n",
    "    \"Idaho\", \"ID\", \"Id\",\n",
    "    \"Illinois\", \"IL\", \"Il\",\n",
    "    \"Indiana\", \"IN\", \"In\",\n",
    "    \"Iowa\", \"IA\", \"Ia\",\n",
    "    \"Kansas\", \"KS\", \"Ks\",\n",
    "    \"Kentucky\", \"KY\", \"Ky\",\n",
    "    \"Louisiana\", \"LA\", \"La\",\n",
    "    \"Maine\", \"ME\", \"Me\",\n",
    "    \"Maryland\", \"MD\", \"Md\",\n",
    "    \"Massachusetts\", \"MA\", \"Ma\",\n",
    "    \"Michigan\", \"MI\", \"Mi\",\n",
    "    \"Minnesota\", \"MN\", \"Mn\",\n",
    "    \"Mississippi\", \"MS\", \"Ms\",\n",
    "    \"Missouri\", \"MO\", \"Mo\",\n",
    "    \"Montana\", \"MT\", \"Mt\",\n",
    "    \"Nebraska\", \"NE\", \"Ne\",\n",
    "    \"Nevada\", \"NV\", \"Nv\",\n",
    "    \"New Hampshire\", \"NH\", \"Nh\",\n",
    "    \"New Jersey\", \"NJ\", \"Nj\",\n",
    "    \"New Mexico\", \"NM\", \"Nm\",\n",
    "    \"New York\", \"NY\", \"Ny\",\n",
    "    \"North Carolina\", \"NC\", \"Nc\",\n",
    "    \"North Dakota\", \"ND\", \"Nd\",\n",
    "    \"Ohio\", \"OH\", \"Oh\",\n",
    "    \"Oklahoma\", \"OK\", \"Ok\",\n",
    "    \"Oregon\", \"OR\", \"Or\",\n",
    "    \"Pennsylvania\", \"PA\", \"Pa\",\n",
    "    \"Rhode Island\", \"RI\", \"Ri\",\n",
    "    \"South Carolina\", \"SC\", \"Sc\",\n",
    "    \"South Dakota\", \"SD\", \"Sd\",\n",
    "    \"Tennessee\", \"TN\", \"Tn\",\n",
    "    \"Texas\", \"TX\", \"Tx\",\n",
    "    \"Utah\", \"UT\", \"Ut\",\n",
    "    \"Vermont\", \"VT\", \"Vt\",\n",
    "    \"Virginia\", \"VA\", \"Va\",\n",
    "    \"Washington\", \"WA\", \"Wa\",\n",
    "    \"West Virginia\", \"WV\", \"Wv\",\n",
    "    \"Wisconsin\", \"WI\", \"Wi\",\n",
    "    \"Wyoming\", \"WY\", \"Wy\",\n",
    "    \"District of Columbia\", \"DC\", \"Dc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfffffae-610b-4ee3-8e32-a1290f4ae290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute lookup tables\n",
    "institution_to_country = dict(zip(institutions_database[\"Institution\"], institutions_database[\"Country\"]))\n",
    "entity_to_country = dict(zip(entities_database[\"name\"], entities_database[\"country code\"]))\n",
    "all_institutions_set = set(institution_to_country.keys())\n",
    "\n",
    "# Instance conerter\n",
    "cc = coco.CountryConverter()\n",
    "\n",
    "# MUST include contact info. Example format: 'Application_name/1.0 (yourEmail@provider.com)'\n",
    "headers = {\n",
    "    'User-Agent': user_agent_NOMINATIM  \n",
    "}\n",
    "\n",
    "gmaps = googlemaps.Client(key=api_key_GOOGLEMAPS)\n",
    "\n",
    "# Compiled regex list if regex matching is required\n",
    "compiled_institution_patterns = [(inst, re.compile(re.escape(inst))) for inst in institution_to_country]\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_country_from_text(text):\n",
    "    \"\"\"Fast country name conversion with cache.\"\"\"\n",
    "    try:\n",
    "        result = cc.convert(names=text, to='name_short')\n",
    "        return result if result != \"not found\" else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_country_with_states(cell_content):\n",
    "    if pd.isna(cell_content):\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        parsed_content = ast.literal_eval(cell_content)\n",
    "        parsed_content_org = [org for org in parsed_content if org[\"label\"] == \"ORG\"]\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "    if not isinstance(parsed_content, list):\n",
    "        return None, None\n",
    "\n",
    "    countries = []\n",
    "    text_blob = []\n",
    "\n",
    "    for el in parsed_content:\n",
    "        if isinstance(el, dict):\n",
    "            text = el.get(\"text\", \"\").strip(\".\") # Remove points as trailing spaces afterbefore texts\n",
    "            label = el.get(\"label\", \"\")\n",
    "\n",
    "            text_blob.append(text)\n",
    "\n",
    "            if text in us_states:\n",
    "                return \"United States\", \"State_in_US\"\n",
    "\n",
    "            if label in {\"LOC\", \"GPE\"}:\n",
    "                country = get_country_from_text(text)\n",
    "                if isinstance(country, str):\n",
    "                    countries.append(country)\n",
    "\n",
    "    if len(countries) == 1:\n",
    "        return countries[0], \"Direct_country\"\n",
    "    elif len(countries) > 1:\n",
    "        # Return most common\n",
    "        try:\n",
    "            return Counter(countries).most_common(1)[0][0], \"Most_common_list_countries\"\n",
    "        except:\n",
    "            with open(DF_input + \"logs errors Most common list countries.txt\", \"a\") as f:\n",
    "                f.write(str(cell_content) + \", \" + str(countries) + \"\\n\")\n",
    "                \n",
    "        \n",
    "    # No matches from LOC/GPE, try ORG matching\n",
    "    entity_text = \" \".join(text_blob)\n",
    "    for el in reversed(parsed_content):\n",
    "        if el.get(\"label\") == \"ORG\":\n",
    "            name = el[\"text\"].strip(\".\")\n",
    "\n",
    "            # Direct match\n",
    "            if name in institution_to_country:\n",
    "                return get_country_from_text(institution_to_country[name]), \"Direct_institution\"\n",
    "\n",
    "            # Fallback regex search\n",
    "            matches = [inst for inst, pattern in compiled_institution_patterns if pattern.search(name)]\n",
    "            if len(matches) == 1:\n",
    "                return get_country_from_text(institution_to_country[matches[0]]), \"Regex_institution\"\n",
    "\n",
    "    # Try entity database\n",
    "    for el in reversed(parsed_content):\n",
    "        name = el.get(\"text\", \"\").strip(\".\")\n",
    "        if name in entity_to_country:\n",
    "            country_code = entity_to_country[name]\n",
    "            country = get_country_from_text(country_code)\n",
    "            if country and re.search(r'\\b' + re.escape(country) + r'\\b', entity_text):\n",
    "                return country, \"Entity_database\"\n",
    "\n",
    "    # If all else fails, try calls to location APIs\n",
    "    # Some calls to Nominatim and Google Maps will be done by passing all text collected by NER. This needs to be prepared\n",
    "    # Merge all texts into one string, in case needed for passing to code below\n",
    "    string_parts = [el[\"text\"] for el in parsed_content if isinstance(el, dict) and \"text\" in el]\n",
    "    string = \", \".join(string_parts) + \", \" if string_parts else \"\"\n",
    "    all_text = string[:-2] # Remove last \" ,\"\n",
    "    \n",
    "    # Prepare for submitting to Nominatim\n",
    "    all_text_submit = quote_plus(all_text)\n",
    "    \n",
    "    \n",
    "    # Other calls will use just the last ORG-labelled text (corresponding to the Institution). This needs to be prepared\n",
    "    if len(parsed_content_org) > 0:\n",
    "        last_org = parsed_content_org[-1]\n",
    "    else:\n",
    "        last_org = \"\"\n",
    "\n",
    "    if isinstance(last_org, str):\n",
    "        last_org_submit = quote_plus(last_org)\n",
    "    else:\n",
    "        last_org_submit = \"\"\n",
    "    \n",
    "    # Call Nominatim with the institution only (last ORG colledted by NER)\n",
    "    result = requests.get(\"https://nominatim.openstreetmap.org/search?q=\"+last_org_submit+\"&format=json&addressdetails=1\", headers=headers)\n",
    "    if result.status_code == 200:\n",
    "        content = json.loads(result.content)\n",
    "    \n",
    "        # If Nominatim retrieves only one item, return its corresponding country\n",
    "        if len(content) == 1:\n",
    "            candidate = content[0][\"address\"][\"country_code\"]\n",
    "            return get_country_from_text(candidate), \"Nominatim\"\n",
    "    \n",
    "        # Otherwise, try with the whole affiliation in Nominatim\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "            result_all = requests.get(\n",
    "                \"https://nominatim.openstreetmap.org/search?q=\" + all_text_submit + \"&format=json&addressdetails=1\", \n",
    "                headers=headers\n",
    "            )\n",
    "            try:\n",
    "                if result_all.content:\n",
    "                    content_all = result_all.json()\n",
    "                else:\n",
    "                    content_all = []\n",
    "            except json.JSONDecodeError:\n",
    "                content_all = []\n",
    "    \n",
    "            # Again, if Nominatim retrieves only one item, return its corresponding country\n",
    "            if len(content_all) == 1:\n",
    "                if \"address\" in content_all[0].keys() and \"country_code\" in content_all[0][\"address\"]:\n",
    "                    candidate = content_all[0][\"address\"][\"country_code\"]\n",
    "                    return get_country_from_text(candidate), \"Nominatim\"\n",
    "    \n",
    "            # Otherwise, try with Google Maps\n",
    "            else:\n",
    "                try:\n",
    "                    # First, try with the whole affiliation\n",
    "                    geocode_result = gmaps.geocode(all_text)\n",
    "                    if geocode_result:\n",
    "                        candidate = [component[\"short_name\"] for component in geocode_result[0][\"address_components\"] if \"country\" in component[\"types\"]]\n",
    "                        # If Google Maps call retrieves only one item, return its associated country \n",
    "                        if len(candidate) == 1:\n",
    "                            return get_country_from_text(candidate[0]), \"Google Maps\"\n",
    "                    \n",
    "                    # Otherwise, try Google Maps only with the last ORG item of the affiliation (institution)\n",
    "                    else:\n",
    "                        try:\n",
    "                            geocode_result = gmapsgeocode(text)\n",
    "                            if geocode_result:\n",
    "                                candidate = [component[\"short_name\"] for component in geocode_result[0][\"address_components\"] if \"country\" in component[\"types\"]]\n",
    "                                # Again, if Google Maps call retrieves only one item, return its associated country \n",
    "                                if len(candidate) == 1:\n",
    "                                    return get_country_from_text(candidate[0]), \"Google Maps\"\n",
    "    \n",
    "                # If all fails, return None\n",
    "                            else:\n",
    "                                return None, None\n",
    "                        except:\n",
    "                            return None, None\n",
    "                except:\n",
    "                    return None, None\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367a5d5e-5b05-4087-babd-72859c7ba5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parsedX_100000.csv',\n",
       " 'parsedX_1000000.csv',\n",
       " 'parsedX_1100000.csv',\n",
       " 'parsedX_1200000.csv',\n",
       " 'parsedX_1300000.csv',\n",
       " 'parsedX_1400000.csv',\n",
       " 'parsedX_1500000.csv',\n",
       " 'parsedX_1600000.csv',\n",
       " 'parsedX_1700000.csv',\n",
       " 'parsedX_1800000.csv',\n",
       " 'parsedX_1900000.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_csvs = []\n",
    "with open(DF_input + \"csv files with Country.txt\", \"r\") as f:\n",
    "    for csv in f:\n",
    "        parsed_csvs.append(csv[:-1])\n",
    "\n",
    "parsed_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b4e0c2-e862-40e3-ba57-1eedd0b28383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 377\n",
      "Parsing csv: parsedX_200000.csv (12/45)\n",
      "--Parsing time: 53023.31\n",
      "Parsing csv: parsedX_2000000.csv (13/45)\n",
      "--Parsing time: 8555.63\n",
      "Parsing csv: parsedX_2100000.csv (14/45)\n",
      "--Parsing time: 8301.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2200000.csv (15/45)\n",
      "--Parsing time: 8193.36\n",
      "Parsing csv: parsedX_2300000.csv (16/45)\n",
      "--Parsing time: 12025.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2400000.csv (17/45)\n",
      "--Parsing time: 11053.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2500000.csv (18/45)\n",
      "--Parsing time: 11310.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2600000.csv (19/45)\n",
      "--Parsing time: 10049.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2700000.csv (20/45)\n",
      "--Parsing time: 10061.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_2800000.csv (21/45)\n",
      "--Parsing time: 10255.66\n",
      "Parsing csv: parsedX_2900000.csv (22/45)\n",
      "--Parsing time: 10300.86\n",
      "Parsing csv: parsedX_300000.csv (23/45)\n",
      "--Parsing time: 34585.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svalb\\AppData\\Local\\Temp\\ipykernel_11512\\2497279153.py:3: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF_input + csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing csv: parsedX_3000000.csv (24/45)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m start = time.time()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mParsing csv: \u001b[39m\u001b[33m\"\u001b[39m + csv + \u001b[33m\"\u001b[39m\u001b[33m (\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(list_csvs.index(csv) + \u001b[32m1\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(n_csvs) + \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df[[\u001b[33m\"\u001b[39m\u001b[33mCountry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCountry_source\u001b[39m\u001b[33m\"\u001b[39m]] = df[\u001b[33m\"\u001b[39m\u001b[33mNER_lastAuthor\u001b[39m\u001b[33m\"\u001b[39m].apply(extract_country_with_states).apply(pd.Series)\n\u001b[32m      7\u001b[39m df.to_csv(DF_input + csv, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(DF_input + \u001b[33m\"\u001b[39m\u001b[33mcsv files with Country.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[32m   4918\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4919\u001b[39m         func,\n\u001b[32m   4920\u001b[39m         convert_dtype=convert_dtype,\n\u001b[32m   4921\u001b[39m         by_row=by_row,\n\u001b[32m   4922\u001b[39m         args=args,\n\u001b[32m   4923\u001b[39m         kwargs=kwargs,\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m     ).apply()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_standard()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = obj._map_values(\n\u001b[32m   1508\u001b[39m     mapper=curried, na_action=action, convert=\u001b[38;5;28mself\u001b[39m.convert_dtype\n\u001b[32m   1509\u001b[39m )\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer(values, mapper, convert=convert)\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mextract_country_with_states\u001b[39m\u001b[34m(cell_content)\u001b[39m\n\u001b[32m    114\u001b[39m     last_org_submit = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Call Nominatim with the institution only (last ORG colledted by NER)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m result = requests.get(\u001b[33m\"\u001b[39m\u001b[33mhttps://nominatim.openstreetmap.org/search?q=\u001b[39m\u001b[33m\"\u001b[39m+last_org_submit+\u001b[33m\"\u001b[39m\u001b[33m&format=json&addressdetails=1\u001b[39m\u001b[33m\"\u001b[39m, headers=headers)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    119\u001b[39m     content = json.loads(result.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m, url, params=params, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m session.request(method=method, url=url, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28mself\u001b[39m.send(prep, **send_kwargs)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = adapter.send(request, **kwargs)\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = conn.urlopen(\n\u001b[32m    668\u001b[39m         method=request.method,\n\u001b[32m    669\u001b[39m         url=url,\n\u001b[32m    670\u001b[39m         body=request.body,\n\u001b[32m    671\u001b[39m         headers=request.headers,\n\u001b[32m    672\u001b[39m         redirect=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    673\u001b[39m         assert_same_host=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    674\u001b[39m         preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    675\u001b[39m         decode_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    676\u001b[39m         retries=\u001b[38;5;28mself\u001b[39m.max_retries,\n\u001b[32m    677\u001b[39m         timeout=timeout,\n\u001b[32m    678\u001b[39m         chunked=chunked,\n\u001b[32m    679\u001b[39m     )\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    788\u001b[39m     conn,\n\u001b[32m    789\u001b[39m     method,\n\u001b[32m    790\u001b[39m     url,\n\u001b[32m    791\u001b[39m     timeout=timeout_obj,\n\u001b[32m    792\u001b[39m     body=body,\n\u001b[32m    793\u001b[39m     headers=headers,\n\u001b[32m    794\u001b[39m     chunked=chunked,\n\u001b[32m    795\u001b[39m     retries=retries,\n\u001b[32m    796\u001b[39m     response_conn=response_conn,\n\u001b[32m    797\u001b[39m     preload_content=preload_content,\n\u001b[32m    798\u001b[39m     decode_content=decode_content,\n\u001b[32m    799\u001b[39m     **response_kw,\n\u001b[32m    800\u001b[39m )\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28msuper\u001b[39m().getresponse()\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         response.begin()\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28mself\u001b[39m._read_status()\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock.recv_into(b)\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read(nbytes, buffer)\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\env_cancer_studies\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for csv in list_csvs:\n",
    "    if csv not in parsed_csvs:\n",
    "        df = pd.read_csv(DF_input + csv)\n",
    "        start = time.time()\n",
    "        print(\"Parsing csv: \" + csv + \" (\" + str(list_csvs.index(csv) + 1) + \"/\" + str(n_csvs) + \")\")\n",
    "        df[[\"Country\", \"Country_source\"]] = df[\"NER_lastAuthor\"].apply(extract_country_with_states).apply(pd.Series)\n",
    "        df.to_csv(DF_input + csv, index=False)\n",
    "        with open(DF_input + \"csv files with Country.txt\", \"a\") as f:\n",
    "            f.write(csv+\"\\n\")\n",
    "        print(\"--Parsing time: \" + str(round(time.time()-start, 2)))\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117edd47-2740-41a7-b9c3-2ee5cf44010a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
